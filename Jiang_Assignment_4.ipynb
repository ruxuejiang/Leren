{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are a supervised learning algorithm used for both classification and regression tasks. For this assignment we are going to take a look at classification. We can use decision trees for issues where we have numerical but also categorical input and target features. The decision tree algorithm you will be making will be based off the *ID3* algorithm as described in chapter 3 of the book \"Machine Learning\" by Tom Mitchell, sections **3.1-3.4**.\n",
    "\n",
    "After building our own *ID3* tree, we will then use scikit-learn to explore numerical univariate decision trees and some possibilities for improving them. Besides working with data containing both categorical and numeric features, this assignment will give us a chance to study the following topics:\n",
    "\n",
    "* Dealing with a more realistic data set. The Iris dataset is useful to get started, as it just works out of the box, but real world datasets will almost always be a lot messier. Having to do some preprocessing to end up with a usable representation is extremely common.\n",
    "* Working with dataframes. `pandas` is a very popular Python library for Data Science that enables you to perform database-like operations on large datasets with great performance. It is a very useful tool to add to your arsenal.\n",
    "* Analysing the results of an algorithm. The results you get when you have (correctly) implemented the algorithm might surprise you. Trying to set up hypotheses about why this is the case and what you could do to improve / prevent / fix this, is a key skill in applying machine learning on real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Before we start with the actual assignment, we will introduce `pandas` through a set of small exercises. For this we will use the book by the creator of `pandas`: \"Python for Data Analysis\". You won't have to read the whole book, but it will serve as a useful reference while you figure out how certain operations are done in `pandas`. We will always refer to the relevant pages, which are included in the *syllabus*.\n",
    "\n",
    "Below each of the exercises is a set of assertions that test whether you gave the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series [1 pt]\n",
    "\n",
    "Start by reading Python for Data Analysis pages 111-115 on Series.\n",
    "\n",
    "Create a `Series`-object named `earnings` containing the following *figures* and using the *sources* as its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales            39041\n",
       "ads               8702\n",
       "subscriptions    13200\n",
       "donations          292\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earnings_sources = [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "earnings_figures = [39041, 8702, 13200, 292]\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "earnings = pd.Series(earnings_figures, index = earnings_sources)\n",
    "earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(earnings) is pd.Series, \"Income has to be a Series\"\n",
    "assert list(earnings.index) == [\"sales\", \"ads\", \"subscriptions\", \"donations\"]\n",
    "assert list(earnings) == [39041, 8702, 13200, 292]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Series`-object named `expenses` using the same format as `earning` with the expenses *figures*. Then create another `Series`-object named `profit` with the profit figures for each category (earnings minus expenses). Lastly, create a variable `total_profit` containing the summed total of `profit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses_sources = [\"ads\", \"sales\", \"donations\", \"subscriptions\"]\n",
    "expenses_figures = [4713, 24282, 0, 3302]\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "expenses = pd.Series(expenses_figures, expenses_sources)\n",
    "profit = earnings - expenses\n",
    "total_profit = profit.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(expenses) is pd.Series\n",
    "assert type(profit) is pd.Series\n",
    "float(total_profit)\n",
    "assert list(profit) == [3989, 292, 14759, 9898]\n",
    "assert total_profit == 28938"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames [1 pt]\n",
    "\n",
    "Next read Python for Data Analysis pages 115-120 on DataFrames.\n",
    "\n",
    "Create a `DataFrame` named `skittles` with the *columns* `amount` and `rating`, using the different colors as the *index*.\n",
    "\n",
    "|&nbsp;      | amount | rating |\n",
    "|------------|--------|--------|\n",
    "| **red**    | 7      | 3      |\n",
    "| **green**  | 4      | 4      |\n",
    "| **blue**   | 6      | 2      |\n",
    "| **purple** | 5      | 4      |\n",
    "| **pink**   | 6      | 3.5    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "### YOUR SOLUTION HERE\n",
    "skittles = pd.DataFrame({'amount':[7,4,6,5,6], 'rating':[3,4,2,4,3.5]},index = ['red','green','blue','purple','pink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(skittles) is DataFrame\n",
    "assert list(skittles.index) == [\"red\", \"green\", \"blue\", \"purple\", \"pink\"]\n",
    "assert list(skittles.columns) == [\"amount\", \"rating\"]\n",
    "assert skittles.loc[\"red\", \"amount\"] == 7\n",
    "assert skittles.loc[\"blue\", \"rating\"] == 2\n",
    "\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x92\\xa6amount\\xa6rating\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x95\\xa3red\\xa5green\\xa4blue\\xa6purple\\xa4pink\\xa8compress\\xc0\\xa6blocks\\x92\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7(\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00\\x0c@\\xa5shape\\x92\\x01\\x05\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7(\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa5shape\\x92\\x01\\x05\\xa5dtype\\xa5int64\\xa5klass\\xa8IntBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(skittles, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average _rating_ of all the skittles and store it in a variable called `skittles_average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "skittles_average = skittles['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(skittles_average)\n",
    "assert skittles_average == 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column to the skittles `DataFrame` called `score`. The score of a color is equal to `amount * rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "skittles['score'] = skittles['amount']*skittles['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"score\" in skittles\n",
    "solution = pd.read_msgpack(b'\\x87\\xa3typ\\xa6series\\xa5klass\\xa6Series\\xa4name\\xa5score\\xa5index\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x95\\xa3red\\xa5green\\xa4blue\\xa6purple\\xa4pink\\xa8compress\\xc0\\xa5dtype\\xa7float64\\xa4data\\xc7(\\x00\\x00\\x00\\x00\\x00\\x00\\x005@\\x00\\x00\\x00\\x00\\x00\\x000@\\x00\\x00\\x00\\x00\\x00\\x00(@\\x00\\x00\\x00\\x00\\x00\\x004@\\x00\\x00\\x00\\x00\\x00\\x005@\\xa8compress\\xc0')\n",
    "pd.testing.assert_series_equal(skittles[\"score\"], solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Indexing [1 pt]\n",
    "\n",
    "Read Python for Data Analysis pages 122-128, Reindexing, Dropping entries, and \"Indexing, selection, and filtering\".\n",
    "\n",
    "Reindex the given `DataFrame` on columns 'a', 'c', and 'e', using indices 10, 20, 50, 60 and store the result in the same `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a     c     e\n",
       "10   0.0   2.0   4.0\n",
       "20   7.0   9.0  11.0\n",
       "50  28.0  30.0  32.0\n",
       "60  35.0  37.0  39.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame = DataFrame(np.arange(6 * 7.).reshape((6, 7)), index=[10, 20, 30, 40, 50, 60], columns=list('abcdefg'))\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "frame = frame.reindex(['a','c','e'], axis=1).loc[[10,20,50,60]]\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(frame) == DataFrame\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x93\\xa1a\\xa1c\\xa1e\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xc7 \\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x002\\x00\\x00\\x00\\x00\\x00\\x00\\x00<\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x91\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x03\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc7\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c@\\x00\\x00\\x00\\x00\\x00\\x00<@\\x00\\x00\\x00\\x00\\x00\\x80A@\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\"@\\x00\\x00\\x00\\x00\\x00\\x00>@\\x00\\x00\\x00\\x00\\x00\\x80B@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00&@\\x00\\x00\\x00\\x00\\x00\\x00@@\\x00\\x00\\x00\\x00\\x00\\x80C@\\xa5shape\\x92\\x03\\x04\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(frame, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all values in the data frame that are *divisible by 3* with the value *0*, and once again store the result in the `frame` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a     b     c     d     e     f     g\n",
       "10   0.0   1.0   2.0   0.0   4.0   5.0   0.0\n",
       "20   7.0   8.0   0.0  10.0  11.0   0.0  13.0\n",
       "30  14.0   0.0  16.0  17.0   0.0  19.0  20.0\n",
       "40   0.0  22.0  23.0   0.0  25.0  26.0   0.0\n",
       "50  28.0  29.0   0.0  31.0  32.0   0.0  34.0\n",
       "60  35.0   0.0  37.0  38.0   0.0  40.0  41.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = DataFrame(np.arange(6 * 7.).reshape((6, 7)), index=[10, 20, 30, 40, 50, 60], columns=list('abcdefg'))\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "frame[frame % 3 == 0] = 0\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(frame) == DataFrame\n",
    "\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x97\\xa1a\\xa1b\\xa1c\\xa1d\\xa1e\\xa1f\\xa1g\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xc70\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x00\\x00\\x00\\x00(\\x00\\x00\\x00\\x00\\x00\\x00\\x002\\x00\\x00\\x00\\x00\\x00\\x00\\x00<\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x91\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x07\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc78\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc8\\x01P\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c@\\x00\\x00\\x00\\x00\\x00\\x00,@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00<@\\x00\\x00\\x00\\x00\\x00\\x80A@\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00 @\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x006@\\x00\\x00\\x00\\x00\\x00\\x00=@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x000@\\x00\\x00\\x00\\x00\\x00\\x007@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80B@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00$@\\x00\\x00\\x00\\x00\\x00\\x001@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00?@\\x00\\x00\\x00\\x00\\x00\\x00C@\\x00\\x00\\x00\\x00\\x00\\x00\\x10@\\x00\\x00\\x00\\x00\\x00\\x00&@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x009@\\x00\\x00\\x00\\x00\\x00\\x00@@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x003@\\x00\\x00\\x00\\x00\\x00\\x00:@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00D@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00*@\\x00\\x00\\x00\\x00\\x00\\x004@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00A@\\x00\\x00\\x00\\x00\\x00\\x80D@\\xa5shape\\x92\\x07\\x06\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(frame, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes this `pandas` introduction. Now lets move on to the actual decision tree assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting heart disease \n",
    "\n",
    "The data set we will be using for this assignment contains heart disease diagnosis results from 4 different hospitals. The data set can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/). Download the dataset, and place the files in the same folder as this notebook.\n",
    "\n",
    "Lets start by looking at the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file, which contains a description of the data set. The file also gives an explanation for the values of the different variables, so when our tree is complete we can interpret the decision rules created by the algorithm. \n",
    "\n",
    "Some variables included here, like *#9 cp: chest pain type*, with 4 labels for different types of chest pain, are clearly categorical. Then there are variables like *#12 chol: serum cholestoral in mg/dl*, containing the concentration of cholesterol, an obvious numeric value. The ability to handle both of these types of data is something not many other machine learning algorithms can do effectively, so in theory a decision tree should be perfect for this data.\n",
    "\n",
    "### Taking a first look [2 pts]\n",
    "\n",
    "Start by downloading the 4 `processed.X.data` files and loading them into a *Pandas DataFrame* with the function `pd.read_csv`. Do not forget to manually set the names of the columns! You can find these names listed in the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file. \n",
    "\n",
    "Create a list `data` that contains these four dataframes and print the dataset. You should see a couple of unexpected values pop up, which probably indicate a missing value. The file describing the data sets states that missing values are indicated by $-9.0$, but this doesn't seem to be the case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[     age  sex  cp trestbps chol fbs  restecg thalach exang oldpeak slope ca  \\\n",
       " 0     63    1   4      140  260   0        1     112     1       3     2  ?   \n",
       " 1     44    1   4      130  209   0        1     127     0       0     ?  ?   \n",
       " 2     60    1   4      132  218   0        1     140     1     1.5     3  ?   \n",
       " 3     55    1   4      142  228   0        1     149     1     2.5     1  ?   \n",
       " 4     66    1   3      110  213   1        2      99     1     1.3     2  ?   \n",
       " ..   ...  ...  ..      ...  ...  ..      ...     ...   ...     ...   ... ..   \n",
       " 195   54    0   4      127  333   1        1     154     0       0     ?  ?   \n",
       " 196   62    1   1        ?  139   0        1       ?     ?       ?     ?  ?   \n",
       " 197   55    1   4      122  223   1        1     100     0       0     ?  ?   \n",
       " 198   58    1   4        ?  385   1        2       ?     ?       ?     ?  ?   \n",
       " 199   62    1   2      120  254   0        2      93     1       0     ?  ?   \n",
       " \n",
       "     thal  num  \n",
       " 0      ?    2  \n",
       " 1      ?    0  \n",
       " 2      ?    2  \n",
       " 3      ?    1  \n",
       " 4      ?    0  \n",
       " ..   ...  ...  \n",
       " 195    ?    1  \n",
       " 196    ?    0  \n",
       " 197    6    2  \n",
       " 198    ?    0  \n",
       " 199    ?    1  \n",
       " \n",
       " [200 rows x 14 columns],\n",
       "      age  sex  cp trestbps chol fbs restecg thalach exang  oldpeak slope ca  \\\n",
       " 0     28    1   2      130  132   0       2     185     0      0.0     ?  ?   \n",
       " 1     29    1   2      120  243   0       0     160     0      0.0     ?  ?   \n",
       " 2     29    1   2      140    ?   0       0     170     0      0.0     ?  ?   \n",
       " 3     30    0   1      170  237   0       1     170     0      0.0     ?  ?   \n",
       " 4     31    0   2      100  219   0       1     150     0      0.0     ?  ?   \n",
       " ..   ...  ...  ..      ...  ...  ..     ...     ...   ...      ...   ... ..   \n",
       " 289   52    1   4      160  331   0       0      94     1      2.5     ?  ?   \n",
       " 290   54    0   3      130  294   0       1     100     1      0.0     2  ?   \n",
       " 291   56    1   4      155  342   1       0     150     1      3.0     2  ?   \n",
       " 292   58    0   2      180  393   0       0     110     1      1.0     2  ?   \n",
       " 293   65    1   4      130  275   0       1     115     1      1.0     2  ?   \n",
       " \n",
       "     thal  num  \n",
       " 0      ?    0  \n",
       " 1      ?    0  \n",
       " 2      ?    0  \n",
       " 3      6    0  \n",
       " 4      ?    0  \n",
       " ..   ...  ...  \n",
       " 289    ?    1  \n",
       " 290    ?    1  \n",
       " 291    ?    1  \n",
       " 292    7    1  \n",
       " 293    ?    1  \n",
       " \n",
       " [294 rows x 14 columns],\n",
       "      age  sex  cp trestbps  chol fbs restecg thalach exang oldpeak slope ca  \\\n",
       " 0     32    1   1       95     0   ?       0     127     0      .7     1  ?   \n",
       " 1     34    1   4      115     0   ?       ?     154     0      .2     1  ?   \n",
       " 2     35    1   4        ?     0   ?       0     130     1       ?     ?  ?   \n",
       " 3     36    1   4      110     0   ?       0     125     1       1     2  ?   \n",
       " 4     38    0   4      105     0   ?       0     166     0     2.8     1  ?   \n",
       " ..   ...  ...  ..      ...   ...  ..     ...     ...   ...     ...   ... ..   \n",
       " 118   70    1   4      115     0   0       1      92     1       0     2  ?   \n",
       " 119   70    1   4      140     0   1       0     157     1       2     2  ?   \n",
       " 120   72    1   3      160     0   ?       2     114     0     1.6     2  2   \n",
       " 121   73    0   3      160     0   0       1     121     0       0     1  ?   \n",
       " 122   74    1   2      145     0   ?       1     123     0     1.3     1  ?   \n",
       " \n",
       "     thal  num  \n",
       " 0      ?    1  \n",
       " 1      ?    1  \n",
       " 2      7    3  \n",
       " 3      6    1  \n",
       " 4      ?    2  \n",
       " ..   ...  ...  \n",
       " 118    7    1  \n",
       " 119    7    3  \n",
       " 120    ?    0  \n",
       " 121    3    1  \n",
       " 122    ?    1  \n",
       " \n",
       " [123 rows x 14 columns],\n",
       "       age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       " 0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       " 1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       " 2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       " 3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       " 4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       " ..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
       " 298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       " 299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       " 300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       " 301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       " 302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       " \n",
       "      slope   ca thal  num  \n",
       " 0      3.0  0.0  6.0    0  \n",
       " 1      2.0  3.0  3.0    2  \n",
       " 2      2.0  2.0  7.0    1  \n",
       " 3      3.0  0.0  3.0    0  \n",
       " 4      1.0  0.0  3.0    0  \n",
       " ..     ...  ...  ...  ...  \n",
       " 298    2.0  0.0  7.0    1  \n",
       " 299    2.0  2.0  7.0    2  \n",
       " 300    2.0  1.0  7.0    3  \n",
       " 301    2.0  1.0  3.0    1  \n",
       " 302    1.0    ?  3.0    0  \n",
       " \n",
       " [303 rows x 14 columns]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "file_names = ['va', 'hungarian', 'switzerland', 'cleveland']\n",
    "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n",
    "va = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data', names=column_names, header=None)\n",
    "hungarian = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data', names=column_names, header=None)\n",
    "switzerland = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data', names=column_names, header=None)\n",
    "cleveland = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', names=column_names, header=None)\n",
    "data = [va, hungarian, switzerland, cleveland]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the scope of this problem by writing the `count_missing` function, which should count the number of missing elements for each feature / column of a data set. The function should thus return a *Series* of missing counts, one count for each column in the data set. We have provided you with the code to print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: va\n",
      "Rows: 200\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0        56     7    7        0       53     53       56    102   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  198   166    0  \n",
      "\n",
      "Filename: hungarian\n",
      "Rows: 294\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         1    23    8        1        1      1        0    190   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  291   266    0  \n",
      "\n",
      "Filename: switzerland\n",
      "Rows: 123\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         2     0   75        1        1      1        6     17   \n",
      "\n",
      "    ca  thal  num  \n",
      "0  118    52    0  \n",
      "\n",
      "Filename: cleveland\n",
      "Rows: 303\n",
      "Missing per column: \n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0    0    0   0         0     0    0        0        0      0        0      0   \n",
      "\n",
      "   ca  thal  num  \n",
      "0   4     2    0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_missing(df):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return df[df=='?'].count()\n",
    "for i in range(len(data)):\n",
    "    print(f'Filename: {file_names[i]}')\n",
    "    print(f'Rows: {len(data[i])}')\n",
    "    \n",
    "    # The following code makes sure that we print horizontally and not vertically\n",
    "    print(f'Missing per column: \\n{count_missing(data[i]).to_frame().T}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data [1 pt]\n",
    "\n",
    "Looking at the results from the previous step, it seems like the sets from some hospitals are more complete than others. There are different approaches you might take to solve this, like replacing the missing values with the average value for that variable, or handling missing values within the algorithm in a seperate way. For now we will take the simplest approach, discarding any rows that contain missing values. This way we only use the complete patient records from each data set. \n",
    "\n",
    "We have concatenated all four DataFrames into a single DataFrame called `df`. Drop any rows containing a missing value. You can use pandas `pd.to_numeric` with keyword `errors='coerce'` to easily convert entries to floats. When trying to convert the missing values, the DataFrame will transform these into `NaN` (Not a Number) instead of a float. After you remove the rows with `NaN`, you should end up with about 300 patient records, most of which are from the Cleveland hospital. Each patient has 14 variables, one of which (the 14th variable) is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.dropna()\n",
    "assert len(df) == 299\n",
    "assert len(df.columns) == 14\n",
    "\n",
    "target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at how how often each label in the target variable actually occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 56, 0: 160, 2: 35, 3: 35, 4: 13})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of each of the labels of our target variable differs greatly. While there are 160 labels with the value $0$, we only have 13 labels with value $4$. In virtually all classification tasks, training data that contains different numbers of representatives from each class might result in a classifier that is biased towards the most common class. When applied to a test set that is similarly imbalanced, this classifier yields an optimistic accuracy estimate. In an extreme case, the classifier might assign every single test case to the majority class, thereby trivially achieving an accuracy equal to the proportion of test cases belonging to the majority class!\n",
    "\n",
    "In the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)) it says *Value 0: < 50% diameter narrowing* and *Value 1: > 50% diameter narrowing*, but it does not explain all the other values. It might be logical to assume that these are different degrees of narrowing, where $0$ would mean no disease and higher values would mean different levels of disease present. Because the distribution of the different values is so skewed however, for now we will just focus on classifying the difference between a value of $0$  and any of the higher values.\n",
    "\n",
    "Change the target column to contain a boolean value that is `True` if there is more than $50\\%$ narrowing and `False` otherwise. Then print the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "df['num'] = df['num'].apply(lambda x: False if x == 0 else True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Decision Trees\n",
    "\n",
    "If you have not yet read sections **3.1-3.4** of \"Machine Learning\" by Tom Mitchell, do so now. The entire chapter 3 on Decision Trees is included in the syllabus.\n",
    "\n",
    "The main idea of decision trees is to find the feature that contains the most \"information\" and then split/group the dataset along the rows of this feature that have the same value. This process of finding the \"most informative\" feature and then splitting is repeated until we arrive at a stopping criterium.\n",
    "\n",
    "We will implement this process over several steps:\n",
    "- Generate splits from a given dataset\n",
    "- Calculate the Shannon entropy; a measure of the amount of information in a given dataset\n",
    "- Calculate the Information Gain for a given split\n",
    "- Combine these metrics into an algorithm that creates the Decision Tree\n",
    "\n",
    "## Splits [3 pts]\n",
    "\n",
    "Before we can determine which of our features produce the most descriptive split, we must create each split. **A split is a grouping of rows in a dataset by each of the unique values in _one_ of the columns.**\n",
    "\n",
    "First, we need to determine what unique values are present in a column. Implement a function named `unique_values` that takes a dataframe `df` and a column name `m` and returns a list of the unique values in the given column. Test the function by entering a couple of the column names and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56. 47. 63. 67. 37. 41. 62. 57. 53. 44. 52. 48. 54. 49. 64. 58. 60. 50.\n",
      " 66. 43. 40. 69. 59. 42. 55. 61. 65. 71. 51. 46. 45. 39. 68. 34. 35. 29.\n",
      " 70. 77. 38. 74. 76.]\n"
     ]
    }
   ],
   "source": [
    "def unique_values(df, m):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return df[m].unique()\n",
    "print(unique_values(df, 'age'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a function named `create_split` that takes a dataframe `df` and a column name `m` and returns a dictionary. This dictionary has the *unique column values* as its keys, and the split dataframes as its values. Remember that you just wrote a function for getting the unique keys! Each of the dataframes consist of only data rows that match with the same unique value. Simply put, the data frame rows are \"grouped\" by each of the different unique values from that column.\n",
    "\n",
    "*Note:* When adding each of the frames in the dictionary, remove the column that was used to create the split. We have already split it into its different unique values, so the information in that column is now redundant and further splits on this column would not be possible.\n",
    "\n",
    "You should end up with a dictionary that has a set of rows for each unique value in the given column. Test the function by entering a column name, and see if the outcome is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(df, m):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    keys = unique_values(df, m)\n",
    "    return {k:df[df[m] == k].drop(columns = m) for k in keys}\n",
    "solution = pd.read_msgpack(b'\\x84\\xa3typ\\xadblock_manager\\xa5klass\\xa9DataFrame\\xa4axes\\x92\\x86\\xa3typ\\xa5index\\xa5klass\\xa5Index\\xa4name\\xc0\\xa5dtype\\xa6object\\xa4data\\x9d\\xa3sex\\xa2cp\\xa8trestbps\\xa4chol\\xa3fbs\\xa7restecg\\xa7thalach\\xa5exang\\xa7oldpeak\\xa5slope\\xa2ca\\xa4thal\\xa3num\\xa8compress\\xc0\\x86\\xa3typ\\xa5index\\xa5klass\\xaaInt64Index\\xa4name\\xc0\\xa5dtype\\xa5int64\\xa4data\\xd8\\x00\\xce\\x02\\x00\\x00\\x00\\x00\\x00\\x00J\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6blocks\\x92\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x0c\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xc7`\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xc7\\xc0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\x80]@\\x00\\x00\\x00\\x00\\x00\\xc0f@\\x00\\x00\\x00\\x00\\x00@j@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0e@\\x00\\x00\\x00\\x00\\x00\\x00h@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00ffffff\\xe6?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\x00\\x00\\x00\\x00\\x00\\x00\\x08@\\xa5shape\\x92\\x0c\\x02\\xa5dtype\\xa7float64\\xa5klass\\xaaFloatBlock\\xa8compress\\xc0\\x86\\xa4locs\\x86\\xa3typ\\xa7ndarray\\xa5shape\\x91\\x01\\xa4ndim\\x01\\xa5dtype\\xa5int64\\xa4data\\xd7\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa8compress\\xc0\\xa6values\\xd5\\x00\\x00\\x00\\xa5shape\\x92\\x01\\x02\\xa5dtype\\xa4bool\\xa5klass\\xa9BoolBlock\\xa8compress\\xc0')\n",
    "pd.testing.assert_frame_equal(create_split(df, 'age')[34], solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy [3 pts]\n",
    "There quite a few different definitions of what entropy is; all of them relate to the notion of chaos / order in a system, but the exact definition strongly depends on the context in which the term is used. Most commonly the term refers to thermodynamic entropy, where it is the describes the number possible configurations a thermodynamic system can have in a specific state. This is related to the idea of a universal entropy, as used in Asimov's classic short story [The Last Question](http://multivax.com/last_question.html). For decision trees we need the information theoretic entropy, or Shannon entropy, which says something about the amount of information contained in a distribution of data. The more ordered or one-sided the distribution is, the less bits we would need on average to express the exact distribution.\n",
    "\n",
    "We will use this measure of entropy to compare the results of decision tree splits to see which is the \"most informative\". For the heart disease problem there are now only 2 class labels we are considering, `True` if *vascular narrowing >= 50% diameter* and `False` otherwise. For a 2 class problem, the entropy is defined as:\n",
    "\n",
    "(3.1) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$ is the ratio between between the labels for class 1 and class 2. \n",
    "\n",
    "First, write a `ratio` function to compute $p$. The function should, given a list of boolean values as class labels, return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contained `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return sum(labels)/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets assume that $p=0$; meaning there are no labels that equal `True`. The computation of $-0\\ log_2(0)$ would then (correctly) result in a math error, however this could also just be defined as having the value $0$ (as it is multiplied by $0$). \n",
    "\n",
    "Write the function `entropy_sub` to compute the value of the log product ($-p\\ log_2(p)$), making sure to return $0$ in the case that $p = 0$. Combine `ratio` and `entropy_sub` to compute the `entropy` of a list of boolean class labels (equation 3.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy_sub(p):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p*math.log(p,2)\n",
    "def entropy(labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    p = ratio(labels)\n",
    "    return entropy_sub(p) + entropy_sub(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `plot_entropy` function to check whether the entropy function works correctly. This function should create many different lists of boolean labels of length N and compute ratio and entropy for each of these lists. Note that for a list of booleans of length 𝑁, there are only 𝑁+1 different possible ratios of labels you need to create. The x-axis of your plot should show the ratios and the y-axis their resulting entropies, which should produce a graph like Figure 3.2 in Mitchell. Show this plot at the end of your code and make sure it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entropy(N):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    ratios = [ratio([True]*i+[False]*(N-i)) for i in range(N+1)]\n",
    "    entropys = [entropy([True]*i+[False]*(N-i)) for i in range(N+1)]\n",
    "    plt.plot(ratios, entropys)\n",
    "### YOUR SOLUTION HERE\n",
    "plot_entropy(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain [3 pts]\n",
    "There are several metrics we can use to asses how good a split is in a decision tree. For this implemenation we will use the *Information Gain*, which is defined as the entropy of the original distribution $\\phi(p)$, minus the entropy of the split distribution $I_m$ resulting from the split on variable $m$.\n",
    "\n",
    "(3.4a) $$IG_m = \\phi(p) - I_m$$\n",
    "\n",
    "Information Gain measures how much the entropy changes from making a specific split, i.e. the gain in predictablity of the data as a result of making a distribution based on a specific variable. When a set of target labels is split on a variable $m$, two or more new lists are created, each with their own entropy. Combining the resulting entropies from a split into $s$ new sets is a simple weighted sum:\n",
    "\n",
    "(3.4b) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "where $N_j$ is the size of the $j^{th}$ split distribution, $p_j$ is the ratio of the target labels for that same $j^{th}$ distribution and $N$ is the size of the distribution before the split. \n",
    "\n",
    "Write the function `split_entropy` to compute $I_m$ for some list of target labels and a given `N`. The `split_labels` argument is a list containing $s$ different lists, each containing the target labels for one part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_entropy(split_labels, N):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    Im = 0\n",
    "    for j in range(len(split_labels)):\n",
    "        Im += len(split_labels[j])*entropy(split_labels[j])\n",
    "    return Im/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our function `create_split` returns a dictionary where the keys are the unique entries present in the column chosen and the values are dataframes that contain the rows that have those unique entries. However, the function `split_entropy` expects a list wherein each element is a list of target labels.\n",
    "\n",
    "Write the function `get_split_labels` that accepts a `split` dictionary and `target` (the name of the column that contains the labels) and returns the `split_labels`. This returned list of lists should be of the same format as before, so it can be used directly as input for the `split_entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_labels(split, target):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return [split[k][target].tolist() for k in split.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `information_gain` function using your earlier functions `entropy` and `split_entropy`. Assume `split_labels` is a list containing $s$ different lists, each containing the target labels from one of each  of the splits. Remember that one split exists of all of the grouped unique values in a column of our data, thus, the total number of target labels given to this function is $N$ and the combination of all labels is $p$. You can use `pd.concat` to \"glue\" the different Series-objects together to compute $p$ directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(split_labels):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    labels = [label for labels in split_labels for label in labels]\n",
    "    return entropy(labels) - split_entropy(split_labels, len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have created two assertions that check whether your functions were implemented correctly. \n",
    "\n",
    "**Explain why these assertions work.**\n",
    "\n",
    "Here we use 'num' as target, then `split` is a dictionary where the keys are True and False and the values are dataframes that contain the rows that have the keys. `split_labels` then is a list of two lists, one only consists of True, the other only False, so the entropy of the split distribution equals to 0 and the information gain equals to the entropy of the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a copy, because we remove the target in create_split, while it is needed for split_labels\n",
    "test_df = df.copy()\n",
    "target_copy = target + '_copy'\n",
    "test_df[target_copy] = test_df[target]\n",
    "\n",
    "split = create_split(test_df, target_copy)\n",
    "split_labels = get_split_labels(split, target)\n",
    "\n",
    "assert entropy(test_df[target].tolist()) == information_gain(split_labels)\n",
    "assert split_entropy(split_labels, len(test_df)) == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3\n",
    "\n",
    "So far, we have determined the total purity of the dataset (entropy), and created the functions that enable us to determine what column of our data is the most informative (Information Gain). That leads us to the introduction of the *ID3* algorithm introduced by Ross Quinlan in 1986. This algorithm can be used to create a categorical decision tree through a process of greedy iterations. Reread table 3.1 in Mitchell's 'Machine Learning', containing the pseudo-code for the algorithm.\n",
    "\n",
    "Our version of the *ID3* algorithm can be described as follows:\n",
    "\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.*\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*\n",
    "\n",
    "We have also provided you with a [namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple) `Node` that can hold the name of the selected split column; `column_name`, the most common label in this node; `mode`, and a dictionary mapping to results of the split; `branches`, wherein the key is each of the 'unique values' possible, and the values are the result of the algorithm on the data available for that unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Node = namedtuple('Node', ['column_name',  'mode', 'branches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Lets take a look at an example before we start writing out the complete algorithm. Say we have the following data set and want to classify on the target column *Vegetable*:\n",
    "\n",
    "|&nbsp;         | Color  | Pits  | Vegetable |\n",
    "|---------------|--------|-------|-----------|\n",
    "| **Broccoli**  | Green  | False | True      |\n",
    "| **Pear**      | Green  | True  | False     |\n",
    "| **Strawberry**| Red    | False | False     |\n",
    "| **Zucchini**  | Green  | False | True      |\n",
    "| **Orange**    | Orange | True  | False     |\n",
    "\n",
    "\n",
    "We won't bother with computing the exact Information Gain here, but looking at the 2 possible columns and the distribution of the target column *Vegetable*, you might conclude that *Color* would be a good variable to split on.\n",
    "\n",
    "We can use the [*namedtuple*](https://docs.python.org/3/library/collections.html#collections.namedtuple) to create a `Node` in our Decision Tree and have it store all the relevant information at that level of the tree. The `column_name` to split on has already been chosen to be *Color* and the `mode` is the most common label of the target column in this node, so this would be *False*, as there are currently more fruits than vegetables in our data set. The `branches` would start out by just being the result of the `create_split` function, meaning there would be 3 new data frames, one for each *Color*. The whole `Node` would then look something like \n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={\n",
    "    \n",
    "            'Green': \n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Broccoli**  | False | True      |\n",
    "| **Pear**      | True  | False     |\n",
    "| **Zucchini**  | False | True      |\n",
    "\n",
    "            'Orange':\n",
    "            \n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Orange**    | True  | False     |\n",
    "\n",
    "            'Red':\n",
    "\n",
    "|&nbsp;         | Pits  | Vegetable |\n",
    "|---------------|-------|-----------|\n",
    "| **Strawberry**| False | False     |\n",
    "            \n",
    "            })\n",
    "    \n",
    "However, this is of course only 1 split of our data. The crucial step in the *ID3* algorithm is step **6**; recursively create the sub-trees. So we want to split each of these data sets again and if those produce new branches, we split those further *again*, etc. This is what creates the actual tree structure, the recursive splitting into nodes, each with new branches.\n",
    "\n",
    "As you hopefully remember from your first year, all recursions need base cases and that is what steps **1 and 2** are for. The *leaf nodes* are reached when we've already perfectly split the data or no more splits are possible, and in those cases we just return the predicted label. The whole purpose of the tree structure is to predict a label for the target column. So when we reach the end of some path in the Decision Tree, all we really care about is what the predicted target label would be at that leaf in the tree, and so that is the only information we will need to store there.\n",
    "\n",
    "Combining these steps, *any* branch in a `Node` should always point to another `Node` or the predicted label for that point in the tree. This means the example structure above, containing 3 data frames, would never actually be returned by the algorithm, as the recursive step to split each frame down to *leaf nodes* is still missing. The result of that would look something like\n",
    "\n",
    "    Node(column='Color', mode=False, branches={\n",
    "    \n",
    "        'Green': Node(column='Pits', mode=True, branches={\n",
    "                \n",
    "                    False: True, \n",
    "\n",
    "                    True: False}),\n",
    "\n",
    "        'Orange': False,\n",
    "        \n",
    "        'Red': False})\n",
    "\n",
    "Note that each terminal value (i.e. leaf node) is now either *True* or *False*, indicating the predicted label for the *Vegetable* column at that point in the tree. If we try and interpret this tree, there is only 1 case where it would predict that something is indeed a vegetable; when it is both *Green* and has *No Pits*. In all other cases this tree would predict it to be a fruit. Verify both the constructed tree and the resulting interpreted rule before moving on to implementation.\n",
    "\n",
    "### Implementation [7 pts]\n",
    "\n",
    "Now that we know what the tree structure will look like, lets build the *ID3* algorithm. So far, we have already included step 5 and 7 for you. Step 5 expects the variables `column_name` and `mode` to already be defined by you, in order to create the `Node`, while the branches start out empty.\n",
    "\n",
    "Implement the other steps and see if your code produces that same tree on `test_df`, which contains the vegetable example. Note that the *white space* was added to make the example more readable, so your actual result should look like:\n",
    "\n",
    "    Node(column_name='Color', mode=False, branches={'Green': Node(column_name='Pits', mode=True, branches={False: True, True: False}), 'Orange': False, 'Red': False})\n",
    "\n",
    "For your convenience we will repeat the steps of the _ID3_ algorithm (this is the same text as you can find above):\n",
    "1. *If the current dataset is pure (all rows have the same label), return the pure label.* Use the `unique_value` function for this.\n",
    "2. *If there are no more possible columns to split; return the most common label.* HINT: You can use pandas [`mode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html).\n",
    "3. *Calculate the Information Gain of each possible split in the dataset.* First create splits for each of the columns in the dataset (except for the target column), then extract the label lists for each of these splits and use these to calculate the Information Gain of that split.\n",
    "4. *Pick the split with the largest Information Gain.*\n",
    "5. *Create a Node for this new/current branch of the tree.*\n",
    "6. *For each of the sub-datasets in the split, create a sub-tree (recursively call `ID3` with the sub-dataset) and use the sub-tree result as the branches of the tree created in step 5.*\n",
    "7. *Return the complete tree.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node(column_name='Color', mode=False, branches={'Green': Node(column_name='Pits', mode=True, branches={False: True, True: False}), 'Red': False, 'Orange': False})\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame([['Green', False, True], ['Green', True, False], ['Red', False, False],\n",
    "                        ['Green', False, True], ['Orange', True, False]],\n",
    "                       index=['Broccoli', 'Pear', 'Strawberry', 'Zucchini', 'Orange'],\n",
    "                       columns=['Color', 'Pits', 'Vegetable'])\n",
    "                       \n",
    "test_target = 'Vegetable'\n",
    "\n",
    "def ID3(data, target):    \n",
    "    # step 1\n",
    "    ### YOUR SOLUTION HERE\n",
    "    if len(unique_values(data, target)) == 1:\n",
    "        return unique_values(data, target)[0]\n",
    "    # step 2\n",
    "    ### YOUR SOLUTION HERE\n",
    "    label = data[target].value_counts().argmax()#.mode().bool()\n",
    "    if data.shape[1] == 1:\n",
    "        return label\n",
    "    # step 3\n",
    "    ### YOUR SOLUTION HERE\n",
    "    splits = []\n",
    "    IGs = []\n",
    "    colnames = [col for col in data.columns if col != target]\n",
    "    for col in colnames:\n",
    "        split = create_split(data, col)\n",
    "        splits.append(split)\n",
    "        split_labels = get_split_labels(split, target)\n",
    "        IGs.append(information_gain(split_labels))\n",
    "    # step 4\n",
    "    ### YOUR SOLUTION HERE\n",
    "    ind = IGs.index(max(IGs))\n",
    "    colname = colnames[ind]\n",
    "    # step 5\n",
    "    tree = Node(column_name = colname, mode = label, branches = splits[ind])    \n",
    "    # step 6\n",
    "    ### YOUR SOLUTION HERE\n",
    "    for k in tree.branches.keys():\n",
    "        tree.branches[k] = ID3(tree.branches[k], target)\n",
    "    # step 7\n",
    "    return tree\n",
    "\n",
    "tree = ID3(test_df, test_target)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have built a tree, it is important to realise that this algorithm *does not* search the entire space of possible trees for the best possible decision tree, but instead opts for the step at each node that will make the most progress at that moment. The tree that results from these locally optimal choices might not be globally optimal (although that happens to be the case for this small toy example).\n",
    "\n",
    "## Classify [2 pts]\n",
    "\n",
    "With this complete tree classifying a new data-entry is pretty easy! Implement the `classify` function, which accepts a tree (a variable containing a `Node` representing the root of the tree), and a *single row* of a DataFrame, i.e. a Series. The function should return the predicted target label for that one row of data.\n",
    "\n",
    "Continuing the example from before; we have a new element from that same data set, but we don't yet know if it would be considered a vegetable, so we want to try and use our decision tree to predict if it is, e.g.\n",
    "\n",
    "|&nbsp;         | Color  | Pits  |\n",
    "|---------------|--------|-------|\n",
    "| **Tomato**    | Red    | False |\n",
    "\n",
    "The function should move through the relevant branches down the tree based on the `column_name` at each node and the corresponding value of the row we are trying to predict. There are 3 options at each branch:\n",
    "\n",
    "1. There is another Node attached to this branch, meaning we should continue further down the tree.\n",
    "2. This branch leads to a *leaf node*, meaning we now have a predicted label for the row we can return.\n",
    "3. The value from the column in our row actually does not have corresponding branch in this node. In that case the function should return the most common label in that node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, row):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    if row[tree.column_name] not in tree.branches.keys():\n",
    "        return tree.mode\n",
    "    elif type(tree.branches[row[tree.column_name]]) == Node:\n",
    "        return classify(tree.branches[row[tree.column_name]], row)\n",
    "    else:\n",
    "        return tree.branches[row[tree.column_name]]\n",
    "\n",
    "test_tomato = pd.Series(['Red', False], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_tomato), \"Tomato is a fruit!\"\n",
    "\n",
    "test_grape = pd.Series(['Blue', True], index=['Color', 'Pits'])\n",
    "assert not classify(tree, test_grape), \"Grape is a fruit!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 3rd option can occur as it is possible the categorical value that is being classified was not present in the training set at that point in the tree. This means that even though there are more branches further down the tree, none of them match the row we are trying to classify. Therefore, the best prediction we can possible give for that row, is simply the most common target label that was present in the training set at that node in the tree.\n",
    "\n",
    "It could be the case that we have never seen this categorical value in our training set before at all, as with the grape example above. However, it is also possible that a specific categorical value *did* occur in our training set, but still some nodes *do not* have a branch for that value.\n",
    "\n",
    "**Explain when this might occur and why returning the most common target label is still a valid solution.**\n",
    "\n",
    "It might occur when feature 1 is splitted first and then in a subtree a node of feature 2 is to be splitted, but the node of feature 2 may not have all categories of feature 2 because when feature 1 is splitted, some of categories of feature 2 are splitted to another subtree.  \n",
    "The nodes which *do not* have a branch for that a specific categorical value could not be the root node, which means there are other more important features. The current note is not as determinant as the nodes closer to the root. So although the current subtree do not have some specific categorical values which are in other subtrees, we still need to make decision according to the decision tree from top to bottom, so returning the most common target label is still a valid solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation [4 pts]\n",
    "\n",
    "With all these elements complete, we can start applying the *ID3* algorithm on the heart disease data set and see how well it can predict the *Vascular narrowing* based on the other features describing each patient.\n",
    "\n",
    "Before we see the accuracy of our Decision Tree, we need to create a validation split. Split the DataFrame into `train` and `test` using a ratio of $0.7$ with [sklearn's `train_test_split` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "train, test = train_test_split(df, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to split the train and test DataFrames into categorical and numerical features, using `cat_num_split`. This could be done manually by taking a good look at the description file ([heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)), but it can also be done by counting the amount of unique values present in a column. As an rule of thumb, it is probably unlikely there are more than 10 different categorical values for any one column, while it is seems very likely that there are more than 10 different numerical values for a column present in the data.\n",
    "\n",
    "Use the `unique_values` function from earlier and select the variables that have no more than the `threshold` argument number of unique values as categorical variables and all others as numerical variables. The categorical variables are all whole numbers, so the resulting 2d-array should be of type `int` and the numeric variables array should of type `float`, for which you can use pandas `astype` method. Remember that both dataframes should have the target column.\n",
    "\n",
    "Apply this function to separate out the categorical and numerical features for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_num_split(df, target, threshold=10):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    cat_name =  [col for col in df.columns if len(unique_values(df, col)) <= threshold and col != target]\n",
    "    num_name = [col for col in df.columns if len(unique_values(df, col)) > threshold and col != target]\n",
    "    df_cat, df_num = df[cat_name].astype(int), df[num_name].astype(float)\n",
    "    df_cat[target], df_num[target] = df[target],  df[target]\n",
    "    return df_cat, df_num\n",
    "train_cat, train_num = cat_num_split(train, target)\n",
    "test_cat, test_num = cat_num_split(test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write the function `validate` that takes a decision tree, a dataframe of testing sample, and the target column name. It should `classify` all rows in the dataframe using the decision tree and return the percentage of elements that was classified correctly. \n",
    "\n",
    "\n",
    "Create a decision tree fitted to the categorical training data created above. Validate the results by computing both the train and test accuracy using the categorical features of the heart disease data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9521531100478469\n",
      "Testing accuracy: 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "def validate(tree, df, target):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    df_features, df_labels = df.drop([target], axis=1), df[target]\n",
    "    prediction = [classify(tree, row) for index, row in df_features.iterrows()]\n",
    "    accuracy = sum(prediction == df_labels)/len(prediction)\n",
    "    return accuracy\n",
    "### YOUR SOLUTION HERE\n",
    "tree = ID3(train_cat, target)\n",
    "print('Training accuracy: ' + str(validate(tree, train_cat, target)))\n",
    "print('Testing accuracy: ' + str(validate(tree, test_cat, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Decision Tree [2 pts]\n",
    "\n",
    "Now that our Categorical Decision Tree is done, we will take a look at a Numerical Decision tree. Start by reading section **3.7.2** from Mitchell. As stated there, it is possible to extend the Categorical Decision Tree to also include numerical boundaries. We could then use this to even select the best Information Gain from *both* the categorical *and* the numerical splits at each node.\n",
    "\n",
    "However, for now we will focus a model that can *only* make numerical splits, as that is what the `scikit-learn` library provides straight out of the box: The [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) is a class that can build a decision tree from numerical data, and so we won't have to implement this tree from scratch.\n",
    "\n",
    "The most important difference between categorical and numerical decision trees is the way the data is split. In categorical data, it is easy to split the tree into the $N$ categories that are present in the data. This is impossible to do in numerical data, as there are infinitely many categories. Categorical Decision trees therefore use binary splits or a so called split boundary; where with each split we create one branch with values smaller than the boundary, and one branch with values greater than or equal to the boundary.\n",
    "\n",
    "This boundary for a column can be determined by trying every possible split boundary available for the set of values. This is done by first sorting the samples and then trying every split half way between two neighbouring values that have different labels. This means there can be as many splits as there are samples, and as such this method is computationally very expensive. An alternative, simpler method that is often used, is trying some amount of random splits for a column and picking the best random split among them.\n",
    "\n",
    "Note that repeated binary splits on the same numerical variable be used to create many different \"decision regions\" for the same variable. For example, consider a label where you want a variable to be above or equal to 3.4, but below 4.8. Here you would need two splits; a first split with 3.4 as the boundary and then in the \"greater equal branch\" of that split, another split boundary of 4.8 on that same variable. This means that while the feature that was split on could be ignored in further splits in the case of a Categorical Decision tree, this is not the case for Numerical Decision Trees, and repeated splits on the same variable can actually greatly improve accuracy.\n",
    "\n",
    "Implement a Numerical Tree Classifier using a `sklearn.tree.DecisionTreeClassifier` and its `fit` and `predict` functions.  Train it using the numerical part of the dataset you split out earlier, and print the training and test accuracies (this can easily be done through the `metrics.accuracy_score` method from the `metrics` module). Note that the `fit` function of the classifier requires you to separate the target column from the dataframe and provide the training set as `X` and `y`; you can use `.drop(target, axis=1)` for this, as it returns a new dataframe, but without the target class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "X, y = train_num.drop([target], axis=1), train_num[target]\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test_num.drop([target], axis=1), test_num[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn Decision Trees implemenation is specifically intended for numerical features. This might not be immideately obvious, but if we read the documentation on [Decision Trees](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) and see what algorithms it uses, we find:\n",
    "\n",
    "```scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.```\n",
    "\n",
    "Even if it does not actually support catagorical data, you can *still* make this implementation work, provided that you give each category a number. **Note: This is not recommended (discouraged even), as this suggests that there is an ordering (the numerical order) to the categorical data.** For many models, saying category 1 is before category 3, and category 2 is somewhere in between them, can really screw up your predictions.\n",
    "\n",
    "Even though this is not recommended, since our data already has integer numbers for each of the categories we could easily try and see what works. Train a Numerical Tree Classifier using `sklearn.DecisionTreeClassifier` using the categorical data, and print the training and test accuracies. Next, train using the complete training data, i.e. categorical and numerical combined and also report the training and testing accuracies.\n",
    "\n",
    "*Note: the linked page on Decision Trees contains more useful information and hints if you plan to use this model on any projects of your own, so it might be worth reading through.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical data:\n",
      "Training accuracy: 0.9521531100478469\n",
      "Testing accuracy: 0.8111111111111111\n",
      "Complete data:\n",
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "print('Categorical data:')\n",
    "X, y = train_cat.drop([target], axis=1), train_cat[target]\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test_cat.drop([target], axis=1), test_cat[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "\n",
    "print('Complete data:')\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test.drop([target], axis=1), test[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis questions [4 pts]\n",
    "\n",
    "Answer these questions about the results. Chapter 3.5 from Mitchell's 'Machine Learning' might contain some hints. Write your answers below each question in this cell.\n",
    "\n",
    "**Why does the training accuracy for numerical splits become 1.0 (i.e. a perfect fit), but remains lower for categorical splits?**\n",
    "\n",
    "The searching for space of hypothese by numerical splits is continuous and precise, it can correctly classifies each of the training data which it is discrete by categorical splits then it cann't fit all the data points.\n",
    "\n",
    "**Explain how it is possible that the testing accuracy using only the categorical variables is higher than the testing accuracy using the categorical and numeric variables combined. What property of the Decision Tree causes this?**\n",
    "\n",
    "It is because of overfitting. When using the categorical and numeric variables combined, the tree becauses really complicated and it also learns the noise from the data and its ability to generalize because very low.  \n",
    "There is a high probability of overfitting in decision tree.\n",
    "\n",
    "**While using categorical data as if it is numerical is generally not recommended, in Decision Trees it doesn't seem as problematic. In fact, we could always reproduce a discrete split using a combination of numeric splits on that same variable. Explain how many splits we would need and what the structure might look like.**\n",
    "\n",
    "We would need `n-1` splits when we have `n` catogories on a variable, then we get `n` continuous intervals.\n",
    "\n",
    "**Aside from just being possible, here the results of using the Numerical Decision Tree of scikit-learn actually seem much better on catagorical data than those of an actual Catagorical Decision Tree. Give a hypothesis on what property of the data might cause this and motivate it with a logical argument.**\n",
    "\n",
    "Maybe the catagorical data has an order, for example, the discrete number could represent degree so that it actually has a similar meaning as continuos variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing overfitting [1 pt]\n",
    "\n",
    "As we now know, trees learn by beginning with **the full training set** and greedily adding conditions that maximize each child nodes' label purity. As the tree grows, each node adds another condition resulting in smaller subgroups. In fact, the number of leaf-nodes grows exponentially as we add more depth to the tree and in just a few splits a training set with thousands of examples can be reduced to a set of leaves with sizes around 1-20, which lack statistical validity. Since our implementation will continue separating nodes until they are either pure or no more conditions can be set to further improve purity, it is very probable that we force our tree to overfit.\n",
    "\n",
    "There are several techniques that deal with tree overfitting. It is possible to prune the tree, for example by removing leaves once the tree is complete, or by preventing it from growing as deep in the first place by setting a threshold on the minimum Information Gain. For more detail on this, read section **3.7.1** from Mitchell's 'Machine Learning'.\n",
    "\n",
    "Scikit-learn's [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) has several argument options to help prevent overfitting. Probably the easiest to get started with is `min_samples_split`, which sets what the minimum number of samples in a node should be before it can be split. If a node has less samples than this number, it will not be split further and the predicted label just becomes the most common label at that node.\n",
    "\n",
    "Try several values for `min_samples_split` and give a value that definitely still overfits, that definitely causes underfitting and the value you think gives the best fit on the data. Report the training and testing accuracy for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting(min_samples_split=2):\n",
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.7666666666666667\n",
      "Underfitting(min_samples_split=100):\n",
      "Training accuracy: 0.784688995215311\n",
      "Testing accuracy: 0.7333333333333333\n",
      "Best fit(min_samples_split=20):\n",
      "Training accuracy: 0.8708133971291866\n",
      "Testing accuracy: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "print('Overfitting(min_samples_split=2):')\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test.drop([target], axis=1), test[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "\n",
    "print('Underfitting(min_samples_split=100):')\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "clf = DecisionTreeClassifier(min_samples_split=100)\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test.drop([target], axis=1), test[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "\n",
    "print('Best fit(min_samples_split=20):')\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "clf = DecisionTreeClassifier(min_samples_split=20)\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "X, y = test.drop([target], axis=1), test[target]\n",
    "y_pred = clf.predict(X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which minimum split size gives the best fit on the data? Explain why you choose this value.**\n",
    "\n",
    "I choose 20 as the best minimum split size since it is a suitable number for the training data which contains 209 rows. It balances the training accuracy and testing accuracy and perfroms well in both training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Decision Tree [1 pt]\n",
    "\n",
    "Sklearn also comes with a tool that can actually plot the whole decision tree. However, plotting the complete tree would be a bit hard to read, as it wouldn't really fit in the figure. We can again use another `DecisionTreeClassifier` argument to limit the size of the tree. Here using `max_depth` makes the most sense, as we want uniformly cut the tree at certain depth for the plot.\n",
    "\n",
    "Fit a decision tree of a limited depth and call it `d_tree`. The code below will already plot it. Make sure it the tree small enough so you can read off the values in the plot. If not, reduce the `max_depth` further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "d_tree = clf.fit(X, y)\n",
    "figure(figsize=(15, 10))\n",
    "tree.plot_tree(d_tree, feature_names=df.columns.values,\n",
    "               class_names=['Healthy', 'Heart Disease'], impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Try to read out the boundaries set in the decision tree. Refer back to the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) if you don't know what a variable means. What parameters seem to be the best indicators for heart disease?**\n",
    "\n",
    "`thal`, `ca`(number of major vessels (0-3) colored by flourosopy),  `cp`(chest pain type) seem to be the best indicators for heart disease.\n",
    "\n",
    "**Explain why considering a very reduced version of the tree should still give some insight in the important factors of heart disease.**\n",
    "\n",
    "The node splitting process of decision tree depends on the information gain. The information gains of factors decrease from top to bottom, so those important factors will be near the root of the tree and we can know them by considering a very reduced version of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests [4 pts]\n",
    "\n",
    "A method for reducing overfitting that is not discussed in Mitchell's book is the ensemble approach. In an ensemble approach multiple models are trained on (variations of) the original dataset, and then used to give a combined prediction. A commonly used ensemble method for decision trees is Random Forest. The essence of Random Forest classification is to have many decision trees that have all been trained on a random subset of features with randomly drawn sample rows. These decision trees then all predict on unseen samples and a majority vote is used as the final prediciton. This procedure usually leads to better model performance; while the predictions of a single tree are highly sensitive to noise in the training data; the average of many trees trained on very similar data is not.\n",
    "\n",
    "We will be implementing a simple but effective variant Random Forest that creates a small sample of all training data and trains several of scikit's `DecisionTreeClassifier`s on a random subset of features. First, we will need to create the forest. Implement `create_forest` that should accept `n_trees` the number of trees that should be in the forest, and returns a list containing all of the `DecisionTreeClassifier` objects. Check the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) for a way to change the maximum amount of features used in the tree to the square root of the number of features and use it when creating the classifiers. *This is an easy way to limit the number of features used in the classifier and will help to differentiate all the models learned by each of the trees.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forest(n_trees):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    return [DecisionTreeClassifier(max_features='sqrt') for i in range(n_trees)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function `train_forest` that accepts a `forest` (a list of `DecisionTreeClassifier` objects) and trains each of the trees with `ratio` samples from `data`. *Training each of the trees on different random subsets of the data will also help to differentiate all the models learned by each of the trees.*\n",
    "\n",
    "Hint: use the pandas `sample` method to create your subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(forest, data, target, ratio=0.5):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    for i in range(len(forest)):\n",
    "        df = data.sample(frac = 0.5)\n",
    "        X, y = df.drop([target], axis=1), df[target]\n",
    "        forest[i] = forest[i].fit(X, y)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `predict_forest` that accepts a `forest` and predicts the target label for the passed `data`.\n",
    "\n",
    "Hint: since we only have two possible predictions, `True` and `False`, which are equal to 1 and 0 respectively, we can just add all predictions of all trees together, adding 1 or 0 for each. If we then divide the total prediction by the number of trees, we end up with an 'average vote' that can be rounded to be either 1 or 0; `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(forest, data):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    votes = np.array([tree.predict(data).astype(int) for tree in forest])\n",
    "    pred = np.mean(votes, axis=0).round()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a random forest with 1000 trees, each using only 3 or 4 random features ($\\approx \\sqrt{13}$) and train each forest with a 50% random sample of the complete training data. Print out the train and testing accuracy. Does this result improve over using just 1 tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "forest = create_forest(1000)\n",
    "forest = train_forest(forest, df, target, ratio=0.5)\n",
    "\n",
    "X, y = train.drop([target], axis=1), train[target]\n",
    "y_pred = predict_forest(forest, X)\n",
    "print('Training accuracy: ' + str(metrics.accuracy_score(y, y_pred)))\n",
    "\n",
    "X, y = test.drop([target], axis=1), test[target]\n",
    "y_pred = predict_forest(forest, X)\n",
    "print('Testing accuracy: ' + str(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This result does improve over using just 1 tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
